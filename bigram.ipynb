{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee0e8b-fb5b-4cbf-a07e-9e53eb4015df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#这个是对的\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",':','-']\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines = fp.read()\n",
    "                    article = []\n",
    "                    text_ = \"\"\n",
    "                    text_ += lines\n",
    "                    article.append(text_)    \n",
    "                        \n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_bigram_vocab(training_set):\n",
    "    one_list = []\n",
    "    article_tokens = []\n",
    "    for instance in training_set:\n",
    "        article = ','.join(instance[0])\n",
    "        article_tokens.append(article)\n",
    "    \n",
    "    paragraph = \",\".join(article_tokens)\n",
    "    one_list.append(paragraph)\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "    text_all_vec = bigram_vectorizer.fit_transform(one_list)\n",
    "    \n",
    "    return bigram_vectorizer\n",
    "\n",
    "def get_bigram_vec(training_set, vectorizer):\n",
    "    vec = vectorizer.transform(training_set).toarray()\n",
    "    \n",
    "    return vec\n",
    "        \n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new\n",
    "\n",
    "\n",
    "def mutual_info_select(X_train,Y_train,num_features):\n",
    "    mfs_sentanalysis = SelectKBest(mutual_info_classif, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = mfs_sentanalysis.transform(X_train)\n",
    "    return mfs_sentanalysis, X_train_new\n",
    "    \n",
    "\n",
    "\n",
    "# def train_svm_classifier(training_set, bigram_vectorizer, chi_num, mutual_num): # Function for training our svm classifier\n",
    "#     X_train=[]\n",
    "#     Y_train=[]\n",
    "#     for instance in training_set:\n",
    "#         vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "#         X_train.append(vector_instance_bigram)\n",
    "#         Y_train.append(instance[1])\n",
    "        \n",
    "#     fs_sentanalysis, X_train_chi_vector = chi_square_select(X_train,Y_train,chi_num)\n",
    "#     mfs_sentanalysis, X_train_mut_vector = mutual_info_select(X_train_chi_vector,Y_train,mutual_num)\n",
    "    \n",
    "#   # Finally, we train the SVM classifier \n",
    "#     svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "#     svm_clf.fit(X_train_mut_vector,Y_train)\n",
    "#     return svm_clf, fs_sentanalysis, mfs_sentanalysis\n",
    "\n",
    "\n",
    "# # print(all_text_list[-1:])\n",
    "# print(all_text_list_[1313])\n",
    "# print(all_label_list_[1313])\n",
    "\n",
    "\n",
    "def train_svm_classifier_test(tra_set,bigram_vectorizer): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in tra_set:\n",
    "        \n",
    "        vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "        X_train = np.append(X_train,vector_instance_bigram)\n",
    "        Y_train = np.append(Y_train,instance[1])\n",
    "        \n",
    "\n",
    "        X_train=np.asarray(X_train)\n",
    "        Y_train=np.asarray(Y_train)\n",
    "\n",
    "    \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(X_train,Y_train)\n",
    "    return svm_clf\n",
    "\n",
    "\n",
    "\n",
    "bigram_vec = get_bigram_vocab(dataset_full)\n",
    "svm_clf = train_svm_classifier_test(new_train_set,bigram_vec)\n",
    "\n",
    "# svm_clf, fs_sentanalysis, mfs_sentanalysis = train_svm_classifier(new_train_set, bigram_vec,500, 250)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   #注意是test_set,上面的描述也错了\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(X_test)\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
