{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98b70791-3661-4107-8c70-4b444321dd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "Size training set: 1780\n",
      "Size test set: 445\n",
      "Size training set: 1780\n",
      "Size test set: 223\n",
      "Size development set: 222\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",]\n",
    "stopwords.extend(new_stopwords)\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    article = fp.readlines()\n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)      \n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "def get_list_tokens(text_list):\n",
    "    paragraph = \",\".join(text_list)\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "    \n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "def get_vector_text(list_vocab,text_list):\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "    paragraph = \",\".join(text_list)\n",
    "    list_tokens_string=get_list_tokens(paragraph)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "\n",
    "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary   #word frequency元组\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_svm_classifier(training_set, vocabulary): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "        X_train.append(vector_instance)\n",
    "        Y_train.append(instance[1])\n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(np.asarray(X_train),np.asarray(Y_train))\n",
    "    return svm_clf\n",
    "\n",
    "vocabulary=get_vocabulary(new_train_set, 1000)\n",
    "svm_clf=train_svm_classifier(new_train_set, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "462e368e-f09c-4a4d-a04b-69f51a5cb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 0 0 0 2 3 1 3 1 3 3 3 4 4 3 0 3 0 4 4 0 4 4 3 1 1 2 2 1 3 0 2 4 3 3 3\n",
      " 3 0 2 3 2 3 3 2 3 0 0 3 3 3 2 3 4 4 3 4 4 2 3 4 1 3 3 1 4 3 3 3 1 4 3 3 3\n",
      " 3 4 0 2 3 4 3 1 2 0 4 1 4 4 0 4 2 0 3 3 3 0 1 0 4 4 0 4 3 1 3 3 0 3 3 3 4\n",
      " 4 4 0 3 0 3 3 0 1 0 3 3 3 2 3 4 3 0 0 3 0 3 2 3 3 0 3 0 3 3 2 0 3 1 0 0 1\n",
      " 4 3 3 2 3 4 3 0 0 3 1 3 2 2 3 3 3 3 4 3 4 0 3 0 4 0 0 0 1 0 3 3 3 2 0 3 0\n",
      " 3 1 2 3 4 3 3 0 3 1 2 4 1 0 4 4 3 0 4 1 4 3 3 3 3 2 3 4 0 0 0 3 0 0 0 0 3\n",
      " 4]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73        57\n",
      "           1       0.48      0.26      0.33        39\n",
      "           2       0.57      0.33      0.42        39\n",
      "           3       0.51      0.80      0.62        56\n",
      "           4       0.53      0.66      0.58        32\n",
      "\n",
      "    accuracy                           0.57       223\n",
      "   macro avg       0.57      0.55      0.54       223\n",
      "weighted avg       0.58      0.57      0.56       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   #注意是test_set,上面的描述也错了\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "Y_text_predictions=svm_clf.predict(X_test)  #重要\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0079199e-c8d9-4469-a3c9-8ecd2b82eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.57\n",
      "Recall: 0.547\n",
      "F1-Score: 0.537\n",
      "Accuracy: 0.574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "\n",
    "precision=precision_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "recall=recall_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "f1=f1_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "accuracy=accuracy_score(Y_test_gold, Y_text_predictions)\n",
    "\n",
    "print (\"Precision: \"+str(round(precision,3)))\n",
    "print (\"Recall: \"+str(round(recall,3)))\n",
    "print (\"F1-Score: \"+str(round(f1,3)))\n",
    "print (\"Accuracy: \"+str(round(accuracy,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daea54d-6a8e-4c79-8d00-dd6118248e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
