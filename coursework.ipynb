{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd23a30-ab4d-4ee2-89c3-6422028f1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "Size training set: 1780\n",
      "Size test set: 445\n",
      "Size training set: 1780\n",
      "Size test set: 223\n",
      "Size development set: 222\n",
      "['Claxton hunting first major medal', '', \"British hurdler Sarah Claxton is confident she can win her first major medal at next month's European Indoor Championships in Madrid.\", '', 'The 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week\\'s Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.', '', 'For the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.']\n",
      "3\n",
      "(['Ad sales boost Time Warner profit', '', 'Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.', '', 'The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.', '', \"Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\", '', 'Time Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.', '', \"TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\"], 0)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18684/1400034643.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_full\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m \u001b[0mbigram_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_bigram_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18684/1400034643.py\u001b[0m in \u001b[0;36mget_bigram_vocab\u001b[1;34m(training_set)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[0mone_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mbigram_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m     \u001b[0mtext_all_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbigram_vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#这个是对的\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines =  fp.readlines()\n",
    "                    article = []    \n",
    "                    for line in lines:\n",
    "                        article.append(line.strip('\\n'))\n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_bigram_vocab(training_set):\n",
    "    one_list = []\n",
    "    article_tokens = []\n",
    "    for instance in training_set:\n",
    "        article = ','.join(instance[0])\n",
    "        article_tokens.append(article)\n",
    "    \n",
    "    paragraph = \",\".join(article_tokens)\n",
    "    one_list.append(paragraph)\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "    text_all_vec = bigram_vectorizer.fit_transform(one_list)\n",
    "    \n",
    "    return bigram_vectorizer\n",
    "\n",
    "def get_bigram_vec(training_set, vectorizer):\n",
    "    vec = vectorizer.transform(training_set).toarray()\n",
    "    return vec\n",
    "        \n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new\n",
    "\n",
    "\n",
    "def mutual_info_select(X_train,Y_train,num_features):\n",
    "    mfs_sentanalysis = SelectKBest(mutual_info_classif, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = mfs_sentanalysis.transform(X_train)\n",
    "    return mfs_sentanalysis, X_train_new\n",
    "    \n",
    "\n",
    "\n",
    "def train_svm_classifier(training_set, bigram_vectorizer, chi_num, mutual_num): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "        X_train.append(vector_instance_bigram)\n",
    "        Y_train.append(instance[1])\n",
    "        \n",
    "    fs_sentanalysis, X_train_chi_vector = chi_square_select(X_train,Y_train,chi_num)\n",
    "    mfs_sentanalysis, X_train_mut_vector = mutual_info_select(X_train_chi_vector,Y_train,mutual_num)\n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(X_train_mut_vector,Y_train)\n",
    "    return svm_clf, fs_sentanalysis, mfs_sentanalysis\n",
    "\n",
    "\n",
    "# print(all_text_list[-1:])\n",
    "print(all_text_list_[1313])\n",
    "print(all_label_list_[1313])\n",
    "\n",
    "\n",
    "# def train_svm_classifier_test(training_set,bigram_vectorizer): # Function for training our svm classifier\n",
    "#     X_train=[]\n",
    "#     Y_train=[]\n",
    "#     for instance in training_set:\n",
    "        \n",
    "#         vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "#         X_train.append(vector_instance_bigram)\n",
    "#         Y_train.append(instance[1])\n",
    "        \n",
    "    \n",
    "#   # Finally, we train the SVM classifier \n",
    "#     svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "#     svm_clf.fit(X_train,Y_train)\n",
    "#     return svm_clf\n",
    "\n",
    "print(dataset_full[0])\n",
    "\n",
    "bigram_vec = get_bigram_vocab(dataset_full)\n",
    "\n",
    "\n",
    "svm_clf, fs_sentanalysis, mfs_sentanalysis = train_svm_classifier(new_train_set, bigram_vec,500, 250)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   #注意是test_set,上面的描述也错了\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(mfs_sentanalysis.transform(fs_sentanalysis.transform(X_test)))\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44406e-d87a-4355-86da-ce44a0b9962d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559bf49-f924-45f1-b5e4-112d02ec2e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f9abf-6199-4f35-9feb-0f976a911086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221f8f5-1b88-4889-a7dc-eab13d69d47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc568ba-6279-4d35-ad8a-54f2ccdb14e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f78401-75e1-4b3f-93d5-cd3ff86de44b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8006d-78bd-4e98-a0e1-5b69adf50cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90efe63-51da-4cf8-b1ce-173dbc5c62f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36349c3e-3476-4e2f-9a38-5ff7e55bfbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328ae04-11e4-4ed5-b1cb-74859e2b1064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30c350-5a51-43e7-a03b-44d8469532bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa45197-a8e2-4be0-8752-c9864d993aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef8d7ea2-f682-4250-a80b-4be5f6302904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Lacroix label bought by US firm\\n', '\\n', 'Luxury goods group LVMH has sold its loss-making Christian Lacroix clothing label to a US investment group.\\n', '\\n', \"The Paris-based firm has been shedding non-core businesses and focusing on its most profitable brands including Moet & Chandon champagne and Louis Vuitton. LVMH said the French designer's haute couture and ready-to-wear labels had been purchased by the Falic Group for an unspecified sum. The Falic Group bought two cosmetics labels from LVMH in 2003. The sale of the Lacroix label comes as many fashion houses are struggling to make money from their expensive haute couture ranges. The Florida-based Falic group, which also runs a chain of 90 duty free stores in the US, said it planned to expand the brand by opening new stores. Mr Lacroix said he planned to stay at the label he founded in 1987 although exact details are still to be confirmed.\\n\"], 0)\n"
     ]
    }
   ],
   "source": [
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, lable in zip(all_text_list, all_lable_list):\n",
    "    dataset_full.append((text,lable))\n",
    "\n",
    "# print(dataset_full[0])\n",
    "print(dataset_full[509])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3938cb9-3ff2-4b79-aebe-00663cb6cbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 1780\n",
      "Size test set: 445\n"
     ]
    }
   ],
   "source": [
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9a14d086-e8fd-4023-84c9-28e3ef5a0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 1780\n",
      "Size test set: 223\n",
      "Size development set: 222\n"
     ]
    }
   ],
   "source": [
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b221d8e3-4d8d-48bd-baa7-1387b41d52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([\"Manufacturing recovery 'slowing'\\n\", '\\n', 'UK manufacturing grew at its slowest pace in one-and-a-half years in January, according to a survey.\\n', '\\n', 'The Chartered Institute of Purchasing and Supply (CIPS) said its purchasing manager index (PMI) fell to 51.8 from a revised 53.3 in December. But, despite missing forecasts of 53.7, the PMI number remained above 50 - indicating expansion in the sector. The CIPS said that the strong pound had dented exports while rising oil and metals prices had kept costs high.\\n', '\\n', 'The survey added that rising input prices and cooling demand had deterred factory managers from hiring new workers in an effort to cut costs. That triggered the second successive monthly fall in the CIPS employment index to 48.3 - its lowest level since June 2003. The survey is more upbeat than official figures - which suggest that manufacturing is in recession - but analysts said the survey did suggest that the manufacturing recovery was running out of steam. \"It appears that the UK is in a two-tier economy again,\" said Prebon Yamane economist Lena Komileva. \"You have weakness in manufacturing, which I think would concern policymakers at the Bank of England.\"\\n'], 0)\n",
      "----------------------------------------------\n",
      "(['Finnan says Irish can win group\\n', '\\n', 'Steve Finnan believes the Republic of Ireland can qualify directly for the World Cup finals.\\n', '\\n', 'After Saturday\\'s superb display in the draw in Paris, Ireland face minnows the Faroe Islands in Dublin on Wednesday. The versatile Finnan, who starred against the French, is confident the group is Ireland\\'s for the taking. \"There is a chance for us now to go on, win our home games and why not win the group, even though it\\'s a tough one,\" said the Liverpool player. Switzerland, Ireland, France and Israel are all now tied on five points from three matches - although the Republic look to have a slight edge after claiming away draws in Basel and Paris. \"In Basel we did not play great football, but when you to go to these places the other teams are going to have the majority of the game. \"In Paris, we looked good throughout the team and a point was the least we deserved because we had a number of chances.\\n', '\\n', '\"Looking back, we had an opportunity to get the three points, but we are happy with a point and that will give us confidence going into Wednesday\\'s game. \"On paper, we have got the toughest matches out of the way and we have set standards for ourselves. \"Automatic qualification is there. It would certainly be good to avoid a play-off, but on the back of a couple of good results I don\\'t see why we can\\'t win the group.\" Manager Brian Kerr was keen to mention the contribution of Stephen Carr and Finnan on Ireland\\'s right flank at the Stade de France. Finnan\\'s normal position is right-back but he looked assured in a more advanced position against the French. \"As I play on the right for my club and being a natural right-back, it was something he (Kerr) looked at because France play strongly down the left-hand side. \"So I was happy to play and Stephen Carr and I enjoyed the game, particularly as the defence and midfield held together well and nullified their attacks.\"\\n'], 3)\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for example in new_train_set[:2]:\n",
    "    print(example)\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bded1d-9bf5-4070-9a72-2de879509f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85653486-4f02-441f-bf47-50293fa99f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b6ffeb-88a8-4e6e-9e25-81d54993b744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ad sales boost Time Warner profit\\n', '\\n', 'Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.\\n', '\\n', 'The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales. TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn. Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.\\n', '\\n', \"Time Warner said on Friday that it now owns 8% of search-engine Google. But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues. It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband. TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.\\n\", '\\n', 'Time Warner\\'s fourth quarter profits were slightly better than analysts\\' expectations. But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results. For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn. \"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said. For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.\\n', '\\n', \"TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators. It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC. The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m. It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\\n\"]\n",
      "0\n",
      "['curry']\n",
      "['curry']\n",
      "1\n",
      "['butler']\n",
      "['butler']\n",
      "['Ad sales boost Time Warner profit\\n,\\n,Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.', ',\\n,The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.', 'TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.', 'Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.', ',\\n,Time Warner said on Friday that it now owns 8% of search-engine Google.', 'But its own internet business, AOL, had has mixed fortunes.', 'It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.', \"However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.\", \"It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband.\", 'TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.', \",\\n,Time Warner's fourth quarter profits were slightly better than analysts' expectations.\", 'But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.', 'For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.', '\"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said.', 'For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.', ',\\n,TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators.', 'It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC.', 'The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m.', \"It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\", 'It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.']\n",
      "['Ad', 'sales', 'boost', 'Time', 'Warner', 'profit', ',', ',', 'Quarterly', 'profits', 'at', 'US', 'media', 'giant', 'TimeWarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'Â£600m', ')', 'for', 'the', 'three', 'months', 'to', 'December', ',', 'from', '$', '639m', 'year-earlier', '.']\n",
      "['ad', 'sale', 'boost', 'time', 'warner', 'profit', ',', ',', 'quarterly', 'profit', 'at', 'us', 'medium', 'giant', 'timewarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'â£600m', ')', 'for', 'the', 'three', 'month', 'to', 'december', ',', 'from', '$', '639m', 'year-earlier', '.']\n"
     ]
    }
   ],
   "source": [
    "# list_tokens=nltk.tokenize.word_tokenize(str(all_text_list[0]))\n",
    "# print(list_tokens)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "paragraph = \",\".join(all_text_list[0])\n",
    "print(all_text_list[0])\n",
    "all_star = [[\"curry\"],[\"butler\"]]\n",
    "for i, word in enumerate(all_star):\n",
    "    print(i)\n",
    "    print(all_star[i])\n",
    "    print(word)\n",
    "# print(paragraph)\n",
    "\n",
    "sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "print(sentence_split)\n",
    "list_tokens_sentence=nltk.tokenize.word_tokenize(sentence_split[0])\n",
    "print(list_tokens_sentence)\n",
    "list_tokens = []\n",
    "for token in list_tokens_sentence:\n",
    "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "print(list_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5816fd7-97ac-4518-9398-fcd1c363888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'sale', 'boost', 'time', 'warner', 'profit', ',', ',', 'quarterly', 'profit', 'at', 'us', 'medium', 'giant', 'timewarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'â£600m', ')', 'for', 'the', 'three', 'month', 'to', 'december', ',', 'from', '$', '639m', 'year-earlier', '.', ',', ',', 'the', 'firm', ',', 'which', 'is', 'now', 'one', 'of', 'the', 'biggest', 'investor', 'in', 'google', ',', 'benefited', 'from', 'sale', 'of', 'high-speed', 'internet', 'connection', 'and', 'higher', 'advert', 'sale', '.', 'timewarner', 'said', 'fourth', 'quarter', 'sale', 'rose', '2', '%', 'to', '$', '11.1bn', 'from', '$', '10.9bn', '.', 'its', 'profit', 'were', 'buoyed', 'by', 'one-off', 'gain', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'warner', 'bros', ',', 'and', 'le', 'user', 'for', 'aol', '.', ',', ',', 'time', 'warner', 'said', 'on', 'friday', 'that', 'it', 'now', 'owns', '8', '%', 'of', 'search-engine', 'google', '.', 'but', 'it', 'own', 'internet', 'business', ',', 'aol', ',', 'had', 'ha', 'mixed', 'fortune', '.', 'it', 'lost', '464,000', 'subscriber', 'in', 'the', 'fourth', 'quarter', 'profit', 'were', 'lower', 'than', 'in', 'the', 'preceding', 'three', 'quarter', '.', 'however', ',', 'the', 'company', 'said', 'aol', \"'s\", 'underlying', 'profit', 'before', 'exceptional', 'item', 'rose', '8', '%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertising', 'revenue', '.', 'it', 'hope', 'to', 'increase', 'subscriber', 'by', 'offering', 'the', 'online', 'service', 'free', 'to', 'timewarner', 'internet', 'customer', 'and', 'will', 'try', 'to', 'sign', 'up', 'aol', \"'s\", 'existing', 'customer', 'for', 'high-speed', 'broadband', '.', 'timewarner', 'also', 'ha', 'to', 'restate', '2000', 'and', '2003', 'result', 'following', 'a', 'probe', 'by', 'the', 'us', 'securities', 'exchange', 'commission', '(', 'sec', ')', ',', 'which', 'is', 'close', 'to', 'concluding', '.', ',', ',', 'time', 'warner', \"'s\", 'fourth', 'quarter', 'profit', 'were', 'slightly', 'better', 'than', 'analyst', \"'\", 'expectation', '.', 'but', 'it', 'film', 'division']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# def get_list_tokens(text_list):\n",
    "#     paragraph = \",\".join(text_list)\n",
    "#     sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "    \n",
    "#     list_tokens=[]\n",
    "#     for sentence in sentence_split:\n",
    "#         list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "#         for token in list_tokens_sentence:\n",
    "#             list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "#     return list_tokens\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# paragraph += \",\".join(all_text_list[0])\n",
    "# list_0 = get_list_tokens(paragraph)\n",
    "# print(list_0)\n",
    "# print(len(all_text_list))\n",
    "\n",
    "def all_tokens(text_list):\n",
    "    list_all = []\n",
    "    for i, word in enumerate(text_list):\n",
    "#     if i == 2:\n",
    "#         break\n",
    "#     else:\n",
    "        paragraph = \",\".join(text_list[i])\n",
    "        list_ = get_list_tokens(paragraph)\n",
    "#         print(paragraph)\n",
    "#         print(list_)\n",
    "#         print(\"------------\" + str(i) + \"-------------\")\n",
    "        \n",
    "        list_all.extend(list_)\n",
    "    return list_all\n",
    "\n",
    "list_0 = all_tokens(all_text_list)\n",
    "print(list_0[0:250])\n",
    "\n",
    "# print(paragraph)\n",
    "# for i in range(len(all_text_list)+1):\n",
    "#     paragraph += \",\".join(all_text_list[i])\n",
    "\n",
    "# print(list_)\n",
    "# print(list_[:250])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a94fee4-c1d5-43a3-a2c1-f964cc6d3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32571\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",]\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "dict_word_frequency={}\n",
    "for word in list_0:\n",
    "    if word in stopwords: continue\n",
    "    if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "    else: dict_word_frequency[word]+=1\n",
    "\n",
    "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(len(sorted_list))\n",
    "#   特征数\n",
    "# print(sorted_list[:15])\n",
    "# vocabulary=[]\n",
    "# for word,frequency in sorted_list:\n",
    "#     vocabulary.append(word)\n",
    "\n",
    "# print(vocabulary[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b87cab8-dc7f-4e9a-b80b-28ed4cc0f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary   #word frequency元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb0559b8-0e6b-4839-8b4f-acce44cf784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   9.   0.   0.   0.   0.   0.   8.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   9.   5.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.  56.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  18.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0. 170.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "def get_vector_text(list_vocab,text_list):\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "    paragraph = \",\".join(text_list)\n",
    "    list_tokens_string=get_list_tokens(paragraph)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "vec = get_vector_text(vocabulary,all_text_list[0])\n",
    "print(vec)\n",
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b14b3c96-7407-455f-a08c-21892977e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16]\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83eae4f0-4c17-4317-91c1-74c68dad2db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stephen curry'], ['micheal jordan']]\n",
      "[['lebron james'], ['kevin durant']]\n",
      "[[['stephen curry'], ['micheal jordan']], [['lebron james'], ['kevin durant']], [['kyre irving'], ['klay tompson']]]\n",
      "3\n",
      "[['stephen curry'], ['micheal jordan']]\n",
      "[['lebron james'], ['kevin durant']]\n",
      "[['kyre irving'], ['klay tompson']]\n"
     ]
    }
   ],
   "source": [
    "# lis = [[\"stephen curry\"],[\"micheal jordan\"],[\"lebron james\"],[\"kevin durant\"],[\"kyre irving\"],[\"klay tompson\"]]\n",
    "# lis_1 = [2,4,6]\n",
    "\n",
    "# list_0 = lis[0:lis_1[0]]\n",
    "# print(list_0)\n",
    "# list_1 = lis[lis_1[0]:lis_1[1]]\n",
    "# print(list_1)\n",
    "\n",
    "\n",
    "# seperate the all_text_list into different list according to different catagories\n",
    "def sep_list(lis,cata):\n",
    "    list_sep = []\n",
    "    for i in range(len(cata)):\n",
    "        if i == 0:\n",
    "            list_sep.append(lis[i:cata[i]])\n",
    "        else:\n",
    "            list_sep.append(lis[cata[i-1]:cata[i]])\n",
    "        \n",
    "    return list_sep\n",
    "    \n",
    "    \n",
    "list_5 = sep_list(lis,lis_1)\n",
    "print(list_5)\n",
    "print(len(lis_1))\n",
    "print(list_5[0])\n",
    "print(lis[2:4])\n",
    "print(lis[4:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fed6ff60-0e1a-4bcf-80ea-e72c7916f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[510, 386, 417, 511, 401]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(catagory_file_num)\n",
    "new_cata = [510,896,1313,1824,2225]\n",
    "\n",
    "print(len(new_cata))\n",
    "sep_list_ = sep_list(all_text_list,new_cata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d8a5abd-48fb-4422-b107-690731f82f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def train_svm_classifier(training_set, vocabulary):\n",
    "\n",
    "\n",
    "# for business_text in sep_list_[0]:\n",
    "#     dataset_full.append((business_text,0))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# X_train=[]\n",
    "# Y_train=[]\n",
    "#    \n",
    "# #      这里要把所有条目都分成类得到dataset_full  然后回头从3-5开始进行数据分类（测试集，验证集等等）但这以下部分已经证实是我们训练用的格式了，有了字典集，就差训练了\n",
    "# for instance in dataset_full:\n",
    "#     vecto = get_vector_text(vocabulary,instance[0])  \n",
    "#     X_train.append(vecto)\n",
    "#     Y_train.append(instance[1])\n",
    "# print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c901f2d-b224-4ff4-9d87-90e6bd1f7690",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17568/1771728938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msvm_clf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_svm_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_train_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "def train_svm_classifier(training_set, vocabulary): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "        X_train.append(vector_instance)\n",
    "        Y_train.append(instance[1])\n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(np.asarray(X_train),np.asarray(Y_train))\n",
    "    return svm_clf\n",
    "\n",
    "\n",
    "vocabulary=get_vocabulary(new_train_set, 1000)\n",
    "\n",
    "svm_clf=train_svm_classifier(new_train_set, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "83c63b8b-fccc-4323-8d37-855834d50e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'sale', 'boost', 'time', 'warner', 'profit', ',', ',', 'quarterly', 'profit', 'at', 'us', 'medium', 'giant', 'timewarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'â£600m', ')', 'for', 'the', 'three', 'month', 'to', 'december', ',', 'from', '$', '639m', 'year-earlier', '.', ',', ',', 'the', 'firm', ',', 'which', 'is', 'now', 'one', 'of', 'the', 'biggest', 'investor', 'in', 'google', ',', 'benefited', 'from', 'sale', 'of', 'high-speed', 'internet', 'connection', 'and', 'higher', 'advert', 'sale', '.', 'timewarner', 'said', 'fourth', 'quarter', 'sale', 'rose', '2', '%', 'to', '$', '11.1bn', 'from', '$', '10.9bn', '.', 'its', 'profit', 'were', 'buoyed', 'by', 'one-off', 'gain', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'warner', 'bros', ',', 'and', 'le', 'user', 'for', 'aol', '.', ',', ',', 'time', 'warner', 'said', 'on', 'friday', 'that', 'it', 'now', 'owns', '8', '%', 'of', 'search-engine', 'google', '.', 'but', 'it', 'own', 'internet', 'business', ',', 'aol', ',', 'had', 'ha', 'mixed', 'fortune', '.', 'it', 'lost', '464,000', 'subscriber', 'in', 'the', 'fourth', 'quarter', 'profit', 'were', 'lower', 'than', 'in', 'the', 'preceding', 'three', 'quarter', '.', 'however', ',', 'the', 'company', 'said', 'aol', \"'s\", 'underlying', 'profit', 'before', 'exceptional', 'item', 'rose', '8', '%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertising', 'revenue', '.', 'it', 'hope', 'to', 'increase', 'subscriber', 'by', 'offering', 'the', 'online', 'service', 'free', 'to', 'timewarner', 'internet', 'customer', 'and', 'will', 'try', 'to', 'sign', 'up', 'aol', \"'s\", 'existing', 'customer', 'for', 'high-speed', 'broadband', '.', 'timewarner', 'also', 'ha', 'to', 'restate', '2000', 'and', '2003', 'result', 'following', 'a', 'probe', 'by', 'the', 'us', 'securities', 'exchange', 'commission', '(', 'sec', ')', ',', 'which', 'is', 'close', 'to', 'concluding', '.', ',', ',', 'time', 'warner', \"'s\", 'fourth', 'quarter', 'profit', 'were', 'slightly', 'better', 'than', 'analyst', \"'\", 'expectation', '.', 'but', 'it', 'film', 'division', 'saw', 'profit', 'slump', '27', '%', 'to', '$', '284m', ',', 'helped', 'by', 'box-office', 'flop', 'alexander', 'and', 'catwoman', ',', 'a', 'sharp', 'contrast', 'to', 'year-earlier', ',', 'when', 'the', 'third', 'and', 'final', 'film', 'in', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'boosted', 'result', '.', 'for', 'the', 'full-year', ',', 'timewarner', 'posted', 'a', 'profit', 'of', '$', '3.36bn', ',', 'up', '27', '%', 'from', 'it', '2003', 'performance', ',', 'while', 'revenue', 'grew', '6.4', '%', 'to', '$', '42.09bn', '.', '``', 'our', 'financial', 'performance', 'wa', 'strong', ',', 'meeting', 'or', 'exceeding', 'all', 'of', 'our', 'full-year', 'objective', 'and', 'greatly', 'enhancing', 'our', 'flexibility', ',', \"''\", 'chairman', 'and', 'chief', 'executive', 'richard', 'parsons', 'said', '.', 'for', '2005', ',', 'timewarner', 'is', 'projecting', 'operating', 'earnings', 'growth', 'of', 'around', '5', '%', ',', 'and', 'also', 'expects', 'higher', 'revenue', 'and', 'wider', 'profit', 'margin', '.', ',', ',', 'timewarner', 'is', 'to', 'restate', 'it', 'account', 'a', 'part', 'of', 'effort', 'to', 'resolve', 'an', 'inquiry', 'into', 'aol', 'by', 'us', 'market', 'regulator', '.', 'it', 'ha', 'already', 'offered', 'to', 'pay', '$', '300m', 'to', 'settle', 'charge', ',', 'in', 'a', 'deal', 'that', 'is', 'under', 'review', 'by', 'the', 'sec', '.', 'the', 'company', 'said', 'it', 'wa', 'unable', 'to', 'estimate', 'the', 'amount', 'it', 'needed', 'to', 'set', 'aside', 'for', 'legal', 'reserve', ',', 'which', 'it', 'previously', 'set', 'at', '$', '500m', '.', 'it', 'intends', 'to', 'adjust', 'the', 'way', 'it', 'account', 'for', 'a', 'deal', 'with', 'german', 'music', 'publisher', 'bertelsmann', \"'s\", 'purchase', 'of', 'a', 'stake', 'in', 'aol', 'europe', ',', 'which', 'it', 'had', 'reported', 'a', 'advertising', 'revenue', '.', 'it', 'will', 'now', 'book', 'the', 'sale', 'of', 'it', 'stake', 'in', 'aol', 'europe', 'a', 'a', 'loss', 'on', 'the', 'value', 'of', 'that', 'stake', '.']\n",
      "['Ad sales boost Time Warner profit\\n,\\n,Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (Â£600m) for the three months to December, from $639m year-earlier.', ',\\n,The firm, which is now one of the biggest investors in Google, benefited from sales of high-speed internet connections and higher advert sales.', 'TimeWarner said fourth quarter sales rose 2% to $11.1bn from $10.9bn.', 'Its profits were buoyed by one-off gains which offset a profit dip at Warner Bros, and less users for AOL.', ',\\n,Time Warner said on Friday that it now owns 8% of search-engine Google.', 'But its own internet business, AOL, had has mixed fortunes.', 'It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters.', \"However, the company said AOL's underlying profit before exceptional items rose 8% on the back of stronger internet advertising revenues.\", \"It hopes to increase subscribers by offering the online service free to TimeWarner internet customers and will try to sign up AOL's existing customers for high-speed broadband.\", 'TimeWarner also has to restate 2000 and 2003 results following a probe by the US Securities Exchange Commission (SEC), which is close to concluding.', \",\\n,Time Warner's fourth quarter profits were slightly better than analysts' expectations.\", 'But its film division saw profits slump 27% to $284m, helped by box-office flops Alexander and Catwoman, a sharp contrast to year-earlier, when the third and final film in the Lord of the Rings trilogy boosted results.', 'For the full-year, TimeWarner posted a profit of $3.36bn, up 27% from its 2003 performance, while revenues grew 6.4% to $42.09bn.', '\"Our financial performance was strong, meeting or exceeding all of our full-year objectives and greatly enhancing our flexibility,\" chairman and chief executive Richard Parsons said.', 'For 2005, TimeWarner is projecting operating earnings growth of around 5%, and also expects higher revenue and wider profit margins.', ',\\n,TimeWarner is to restate its accounts as part of efforts to resolve an inquiry into AOL by US market regulators.', 'It has already offered to pay $300m to settle charges, in a deal that is under review by the SEC.', 'The company said it was unable to estimate the amount it needed to set aside for legal reserves, which it previously set at $500m.', \"It intends to adjust the way it accounts for a deal with German music publisher Bertelsmann's purchase of a stake in AOL Europe, which it had reported as advertising revenue.\", 'It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# paragraph = \",\".join(all_text_list[0])\n",
    "# print(all_text_list[0])\n",
    "# print(\"-----------------------\")\n",
    "# all_star = [[\"curry\"],[\"butler\"]]\n",
    "# for i, word in enumerate(all_star):\n",
    "#     print(i)\n",
    "#     print(all_star[i])\n",
    "#     print(word)\n",
    "# print(paragraph)\n",
    "\n",
    "# sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "# print(sentence_split)\n",
    "# list_tokens_sentence=nltk.tokenize.word_tokenize(sentence_split[0])\n",
    "# print(list_tokens_sentence)\n",
    "# list_tokens = []\n",
    "# for token in list_tokens_sentence:\n",
    "#       list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "# print(list_tokens)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_list_tokens(text_list):\n",
    "    paragraph = \",\".join(text_list)\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "    \n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "new = get_list_tokens(all_text_list[0])\n",
    "print(new)\n",
    "\n",
    "def get_para(text_list):\n",
    "    paragraph = \",\".join(text_list)\n",
    "    sentence_split = nltk.tokenize.sent_tokenize(paragraph)\n",
    "    return sentence_split\n",
    "\n",
    "para = get_para(all_text_list[0])\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84da617f-addd-4df3-8a36-0cf71cc6f62c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_text_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13076/1947119214.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mall_text_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_text_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_text_list' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "all_text_list[0]\n",
    "print(all_text_list[0])\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "print(vectorizer)\n",
    "X = vectorizer.fit_transform(all_text_list[0])\n",
    "vec = vectorizer.get_feature_names_out()\n",
    "print(vec[:50])\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab77b875-4e85-4847-805a-78984b518b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 1 0 ... 0 0 3]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ac570-56cd-4d72-8f12-b9c5b4f7d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "...     'This is the first document.',\n",
    "...     'This document is the second document.',\n",
    "...     'And this is the third one.',\n",
    "...     'Is this the first document?',\n",
    "... ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ab8466de-48b3-4982-891b-b894fe5b4833",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[\"Manufacturing recovery 'slowing'\\n\" '\\n'\n 'UK manufacturing grew at its slowest pace in one-and-a-half years in January, according to a survey.\\n'\n '\\n'\n 'The Chartered Institute of Purchasing and Supply (CIPS) said its purchasing manager index (PMI) fell to 51.8 from a revised 53.3 in December. But, despite missing forecasts of 53.7, the PMI number remained above 50 - indicating expansion in the sector. The CIPS said that the strong pound had dented exports while rising oil and metals prices had kept costs high.\\n'\n '\\n'\n 'The survey added that rising input prices and cooling demand had deterred factory managers from hiring new workers in an effort to cut costs. That triggered the second successive monthly fall in the CIPS employment index to 48.3 - its lowest level since June 2003. The survey is more upbeat than official figures - which suggest that manufacturing is in recession - but analysts said the survey did suggest that the manufacturing recovery was running out of steam. \"It appears that the UK is in a two-tier economy again,\" said Prebon Yamane economist Lena Komileva. \"You have weakness in manufacturing, which I think would concern policymakers at the Bank of England.\"\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17568/1654111830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_train_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mchi_vector_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchi_square_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17568/1654111830.py\u001b[0m in \u001b[0;36mchi_square_select\u001b[1;34m(X_train, Y_train)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mchi_square_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mfs_sentanalysis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mX_train_sentanalysis_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfs_sentanalysis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_sentanalysis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train_sentanalysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \"\"\"\n\u001b[1;32m--> 397\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    398\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 964\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m    965\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# If input is 1D raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 769\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    770\u001b[0m                     \u001b[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[\"Manufacturing recovery 'slowing'\\n\" '\\n'\n 'UK manufacturing grew at its slowest pace in one-and-a-half years in January, according to a survey.\\n'\n '\\n'\n 'The Chartered Institute of Purchasing and Supply (CIPS) said its purchasing manager index (PMI) fell to 51.8 from a revised 53.3 in December. But, despite missing forecasts of 53.7, the PMI number remained above 50 - indicating expansion in the sector. The CIPS said that the strong pound had dented exports while rising oil and metals prices had kept costs high.\\n'\n '\\n'\n 'The survey added that rising input prices and cooling demand had deterred factory managers from hiring new workers in an effort to cut costs. That triggered the second successive monthly fall in the CIPS employment index to 48.3 - its lowest level since June 2003. The survey is more upbeat than official figures - which suggest that manufacturing is in recession - but analysts said the survey did suggest that the manufacturing recovery was running out of steam. \"It appears that the UK is in a two-tier economy again,\" said Prebon Yamane economist Lena Komileva. \"You have weakness in manufacturing, which I think would concern policymakers at the Bank of England.\"\\n'].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce284c-d36b-40fa-955f-7340a5c40593",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
