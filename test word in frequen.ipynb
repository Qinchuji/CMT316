{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e49221-6150-430e-af6e-a6e21b08743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "Size training set: 1113\n",
      "Size test set: 1112\n",
      "Size training set: 1113\n",
      "Size test set: 556\n",
      "Size development set: 556\n"
     ]
    }
   ],
   "source": [
    "# 最正确\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",':','-']\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path) # get bbc file\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines = fp.read()\n",
    "                    article = []\n",
    "                    text_ = \"\"\n",
    "                    text_ += lines\n",
    "                    article.append(text_)    \n",
    "                        \n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)      \n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.5)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "def get_list_tokens(text_list):\n",
    "    paragraph = \",\".join(text_list)\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "    \n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "def get_vector_text(list_vocab,text_list):\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "    paragraph = \",\".join(text_list)\n",
    "    list_tokens_string=get_list_tokens(paragraph)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "\n",
    "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary   #word frequency元组\n",
    "\n",
    "\n",
    "def get_bigram_vocab(training_set):\n",
    "    one_list = []\n",
    "    article_tokens = []\n",
    "    for instance in training_set:\n",
    "        article = ','.join(instance[0])\n",
    "        article_tokens.append(article)\n",
    "    \n",
    "    paragraph = \",\".join(article_tokens)\n",
    "    one_list.append(paragraph)\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "    text_all_vec = bigram_vectorizer.fit_transform(one_list)\n",
    "    \n",
    "    return bigram_vectorizer\n",
    "\n",
    "def get_bigram_vec(training_set, vectorizer):\n",
    "    vec = bigram_vectorizer.transform(training_set).toarray()\n",
    "    return vec\n",
    "        \n",
    "    \n",
    "\n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_svm_classifier(training_set, vocabulary, chi_num): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "        \n",
    "        X_train.append(vector_instance)\n",
    "        \n",
    "        Y_train.append(instance[1])\n",
    "        \n",
    "    fs_sentanalysis, X_train_chi_vector = chi_square_select(X_train,Y_train,chi_num)\n",
    "    \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(X_train_chi_vector,Y_train)\n",
    "    return svm_clf, fs_sentanalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def train_svm_classifier_test(training_set,bigram_vectorizer): # Function for training our svm classifier\n",
    "#     X_train=[]\n",
    "#     Y_train=[]\n",
    "#     for instance in training_set:\n",
    "        \n",
    "#         vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "#         X_train.append(vector_instance_bigram)\n",
    "#         Y_train.append(instance[1])\n",
    "        \n",
    "    \n",
    "#   # Finally, we train the SVM classifier \n",
    "#     svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "#     svm_clf.fit(X_train,Y_train)\n",
    "#     return svm_clf\n",
    "\n",
    "\n",
    "\n",
    "# bigram_vec = get_bigram_vocab(dataset_full)\n",
    "# svm_clf = train_svm_classifier_test(new_train_set, bigram_vec)\n",
    "\n",
    "vocabulary = get_vocabulary(dataset_full, 100)\n",
    "svm_clf, fs_sentanalysis = train_svm_classifier(new_train_set, vocabulary,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3c33e5-f8da-4793-8dec-3b3a4e4282ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 2 3 0 3 4 3 3 3 3 3 3 0 3 3 3 4 2 0 3 4 4 0 0 3 1 3 3 3 3 0 0 3 3 0 3\n",
      " 3 3 0 0 3 0 0 1 3 3 1 0 3 3 0 3 0 1 3 3 0 0 0 1 3 3 3 3 3 3 1 3 3 0 3 3 3\n",
      " 0 3 3 0 0 0 3 3 3 3 3 0 2 3 0 2 3 0 0 3 2 3 3 0 3 3 3 3 3 3 3 0 0 3 3 3 2\n",
      " 3 3 3 3 4 0 3 3 0 3 3 3 2 3 0 3 3 0 2 1 0 3 3 3 0 0 2 2 4 3 3 2 3 2 0 3 3\n",
      " 3 0 3 0 3 0 3 3 3 3 0 0 3 3 0 2 3 3 3 3 3 3 3 3 3 3 4 1 3 3 3 4 0 3 3 2 3\n",
      " 3 0 3 4 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 3 3 3 3 3 3 3 3 3 3 2 2 4 3 2\n",
      " 3 1 3 3 3 0 2 0 3 3 0 3 3 4 3 4 3 3 3 0 2 0 3 3 3 3 3 3 3 3 3 0 3 3 0 3 3\n",
      " 3 0 0 3 0 3 0 2 1 3 2 0 3 3 3 3 0 0 0 3 3 0 0 3 0 3 0 3 3 3 3 4 0 3 3 0 3\n",
      " 2 2 3 2 0 3 2 3 2 3 3 4 3 3 2 3 3 3 3 3 1 2 3 4 3 0 3 3 3 3 3 0 3 3 3 1 3\n",
      " 3 3 3 0 3 0 3 3 3 4 3 0 3 3 3 0 3 2 0 3 3 0 3 3 0 0 3 3 2 2 0 3 0 3 3 0 3\n",
      " 3 2 3 3 3 3 3 3 3 3 3 0 1 3 3 3 0 3 0 3 0 3 3 3 0 2 3 0 3 3 1 3 3 0 3 1 0\n",
      " 3 3 3 3 3 3 3 0 0 3 2 3 3 0 3 3 3 3 3 3 3 3 0 3 3 3 0 3 0 0 3 3 3 3 2 3 3\n",
      " 0 3 3 3 2 0 2 3 3 0 2 1 3 3 1 3 3 3 3 3 0 1 3 3 3 3 0 3 0 3 3 3 3 3 3 3 3\n",
      " 3 3 0 3 3 3 2 3 3 3 3 3 3 2 0 3 0 4 0 3 3 3 0 3 0 3 3 0 3 0 3 0 0 3 3 0 2\n",
      " 3 3 0 3 1 0 3 2 3 1 3 3 0 2 3 0 3 3 0 3 3 3 3 0 3 2 0 0 4 3 3 2 0 3 3 0 0\n",
      " 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.68      0.66       121\n",
      "           1       0.15      0.03      0.05       106\n",
      "           2       0.22      0.10      0.14        98\n",
      "           3       0.37      0.97      0.53       130\n",
      "           4       0.33      0.06      0.10       101\n",
      "\n",
      "    accuracy                           0.41       556\n",
      "   macro avg       0.34      0.37      0.30       556\n",
      "weighted avg       0.35      0.41      0.32       556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   #注意是test_set,上面的描述也错了\n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(fs_sentanalysis.transform(X_test))\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e83c23-27d5-4d19-aa49-fc46b582c88e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
