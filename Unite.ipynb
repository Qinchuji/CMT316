{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f23787d3-17a2-4737-90b8-ee15a983a05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "Size training set: 445\n",
      "Size test set: 1780\n",
      "Size training set: 445\n",
      "Size test set: 890\n",
      "Size development set: 890\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",':','-']\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path) # get bbc file\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines = fp.read()\n",
    "                    article = []\n",
    "                    text_ = \"\"\n",
    "                    text_ += lines\n",
    "                    article.append(text_)    \n",
    "                        \n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)      \n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.8)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "def get_list_tokens(text_list):\n",
    "    paragraph = \",\".join(text_list)\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(paragraph)\n",
    "    \n",
    "    list_tokens=[]\n",
    "    for sentence in sentence_split:\n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "def get_vector_text(list_vocab,text_list):\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "    paragraph = \",\".join(text_list)\n",
    "    list_tokens_string=get_list_tokens(paragraph)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n",
    "\n",
    "\n",
    "def get_vocabulary(training_set, num_features): # Function to retrieve vocabulary\n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary   #word frequency \n",
    "\n",
    "\n",
    "def get_bigram_vocab(training_set):\n",
    "    one_list = []\n",
    "    article_tokens = []\n",
    "    for instance in training_set:\n",
    "        article = ','.join(instance[0])\n",
    "        article_tokens.append(article)\n",
    "    \n",
    "    paragraph = \",\".join(article_tokens)\n",
    "    one_list.append(paragraph)\n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "    text_all_vec = bigram_vectorizer.fit_transform(one_list)\n",
    "    \n",
    "    return bigram_vectorizer\n",
    "\n",
    "def get_bigram_vec(training_set, vectorizer):\n",
    "    vec = bigram_vectorizer.transform(training_set).toarray()\n",
    "    return vec\n",
    "\n",
    "\n",
    "def get_hash_vec(training_set):\n",
    "    \n",
    "    hashing_vectorizer = HashingVectorizer(n_features = 20)\n",
    "    text_all_vec = hashing_vectorizer.transform(training_set).toarray()\n",
    "    \n",
    "    return text_all_vec\n",
    "    \n",
    "\n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new\n",
    "\n",
    "def append_list(list_0,list_1):\n",
    "    vec_mul= np.append(list_0,list_1)\n",
    "    return vec_mul\n",
    "\n",
    "def minmax_scale(list_):\n",
    "    \n",
    "    list_ = list_.reshape(-1,1)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(list_)\n",
    "    new_mul_fea = scaler.transform(list_)\n",
    "    return new_mul_fea\n",
    "\n",
    "def train_svm_classifier(training_set, vocabulary, chi_num): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        \n",
    "        vector_fre = get_vector_text(vocabulary,instance[0])\n",
    "        vector_hash = get_hash_vec(instance[0])\n",
    "        vec_mul = append_list(vector_hash, vector_fre)\n",
    "        vec_mul = minmax_scale(vec_mul)\n",
    "        \n",
    "        X_train.append(vec_mul)\n",
    "        \n",
    "        Y_train.append(instance[1])\n",
    "    \n",
    "    X_train = np.asarray(X_train)\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    \n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    d2_X_train = X_train.reshape((nsamples,nx*ny))\n",
    "    \n",
    "    fs_sentanalysis, X_train_chi_vector = chi_square_select(d2_X_train,Y_train,chi_num)\n",
    "    \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(X_train_chi_vector,Y_train)\n",
    "    return svm_clf, fs_sentanalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabulary = get_vocabulary(new_train_set, 10)\n",
    "svm_clf, fs_sentanalysis = train_svm_classifier(new_train_set, vocabulary,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2043b36d-f042-4445-9b6a-e4fd0d2d2b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 3 3 2 2 4 0 2 3 3 1 3 3 3 3 0 3 3 3 1 0 3 1 2 3 4 4 4 3 2 2 4 0 3 3\n",
      " 0 2 1 3 2 2 3 0 2 0 2 0 3 1 3 3 1 0 3 0 4 0 4 2 0 2 1 2 2 3 3 3 1 3 2 4 3\n",
      " 4 0 0 4 0 3 0 0 2 1 1 1 3 3 1 3 1 4 3 3 3 4 0 4 0 3 0 4 2 3 1 3 0 0 4 0 3\n",
      " 3 0 3 3 3 0 0 1 0 3 3 0 3 3 0 1 4 2 4 2 2 3 4 1 0 0 2 0 3 1 3 4 0 0 0 2 3\n",
      " 0 3 0 0 3 3 3 3 0 0 3 2 0 1 3 4 4 2 1 3 0 4 2 2 0 0 1 3 1 3 3 0 2 1 4 1 2\n",
      " 0 2 1 1 0 0 3 4 0 3 0 0 3 0 1 3 3 0 3 3 2 3 0 3 3 1 2 0 4 2 3 3 2 3 3 0 1\n",
      " 0 3 4 3 1 4 2 0 4 3 1 0 0 4 1 4 2 3 2 2 1 3 0 3 3 4 3 2 3 3 3 3 4 1 2 3 0\n",
      " 1 3 1 0 2 0 0 4 0 0 4 3 0 1 0 1 0 1 2 0 3 1 2 1 1 0 0 4 3 2 0 4 0 1 0 3 0\n",
      " 2 1 0 4 2 1 4 3 2 3 3 0 3 1 3 0 1 0 3 4 3 2 0 1 3 4 2 0 0 0 3 1 0 2 0 3 3\n",
      " 1 0 3 2 2 3 0 3 4 2 0 0 0 0 2 0 3 0 2 2 0 2 0 0 2 0 1 0 3 4 0 2 0 3 3 1 3\n",
      " 2 4 0 1 1 3 0 3 4 4 3 1 4 0 3 0 1 0 1 0 2 3 0 4 2 4 3 0 1 1 3 2 2 4 3 2 4\n",
      " 2 4 3 2 2 0 0 0 4 4 4 3 3 3 1 0 0 3 0 1 0 2 1 4 2 3 3 1 0 0 3 3 2 1 4 4 0\n",
      " 2 4 1 2 3 0 4 1 0 2 3 4 4 3 0 2 2 3 4 2 4 1 1 2 0 1 2 3 3 2 1 3 1 0 2 0 1\n",
      " 0 3 3 3 1 2 1 0 3 4 4 0 0 3 2 0 0 3 2 4 3 0 3 3 0 0 3 4 4 3 0 3 4 3 1 0 3\n",
      " 3 2 3 3 1 3 2 0 3 4 4 4 1 0 1 3 3 4 1 0 3 3 3 0 0 3 4 4 3 3 0 1 4 4 0 2 1\n",
      " 1 2 1 3 3 1 0 0 1 3 3 0 3 4 3 4 3 0 2 0 4 4 3 1 0 3 3 3 2 0 0 4 3 3 3 0 2\n",
      " 3 3 0 1 3 3 2 4 3 4 0 3 1 0 2 1 2 0 3 4 0 1 2 2 0 0 2 3 0 1 3 0 1 1 4 1 1\n",
      " 1 3 1 0 4 3 3 3 0 3 0 3 3 4 4 1 2 0 2 0 3 0 4 1 0 1 3 1 3 1 3 3 3 1 0 3 1\n",
      " 3 4 0 3 3 4 0 0 3 4 2 3 3 1 4 3 0 3 2 0 3 1 0 3 1 0 3 4 3 2 3 3 1 1 0 3 3\n",
      " 1 3 4 3 0 0 3 0 3 0 0 3 1 0 2 2 2 3 3 3 2 2 0 2 2 3 3 0 4 3 1 3 4 2 0 3 4\n",
      " 3 0 3 0 4 4 3 3 0 3 1 1 3 0 0 0 0 2 0 2 1 1 2 3 1 4 1 3 0 3 1 2 3 2 1 3 0\n",
      " 0 3 1 3 1 4 0 0 4 0 1 0 0 3 0 1 3 0 0 3 0 1 3 4 0 0 3 1 3 0 0 4 3 1 3 0 0\n",
      " 3 3 0 3 2 4 2 0 3 0 0 3 3 4 3 2 1 3 1 0 3 3 3 1 2 0 1 0 3 3 3 2 3 2 0 2 1\n",
      " 0 1 3 2 2 3 0 1 1 3 3 0 4 2 0 2 0 0 4 3 4 1 4 4 2 3 3 0 3 1 4 4 4 1 1 4 3\n",
      " 1 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.63      0.57       188\n",
      "           1       0.47      0.40      0.43       167\n",
      "           2       0.61      0.44      0.51       178\n",
      "           3       0.45      0.63      0.52       196\n",
      "           4       0.33      0.24      0.28       161\n",
      "\n",
      "    accuracy                           0.48       890\n",
      "   macro avg       0.48      0.47      0.46       890\n",
      "weighted avg       0.48      0.48      0.47       890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   \n",
    "    vector_fre = get_vector_text(vocabulary,instance[0])\n",
    "    vector_hash = get_hash_vec(instance[0])\n",
    "    vec_mul_test = append_list(vector_hash, vector_fre)\n",
    "    vec_mul_test = minmax_scale(vec_mul_test)\n",
    "    \n",
    "    X_test.append(vec_mul_test)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "nsamples, nx, ny = X_test.shape\n",
    "d2_X_test = X_test.reshape((nsamples,nx*ny))\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(fs_sentanalysis.transform(d2_X_test))\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef1148-0137-4be6-88ac-ee10edd64db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def append_list(list_0,list_1):\n",
    "#     vec_mul= np.append(list_0,list1)\n",
    "#     return vec_mul\n",
    "# print(vec_ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed906b40-12f8-47c6-be43-e2d0a0e06f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
