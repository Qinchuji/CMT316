{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aaa1245-bef8-4d7d-9b5b-15002a4f8eb5",
   "metadata": {},
   "source": [
    "This is a supervised multi-class text classification problem, in order to train the model to predict the correct text for each category they belong to, I used an SVM classifier as my model, and word frequency as the basic feature selection method accompanied by chi-square statistic and mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4832415-ec1f-454c-9d9f-3a785b4b30fc",
   "metadata": {},
   "source": [
    "## PREPARATION\n",
    "\n",
    "---\n",
    "\n",
    "import all the libraries that we are going to use, including as usual numpy (vector manipulation), nltk (text preprocessing), scikit-learn (machine learning), os (pathname manipulations), random(shuffle dataset), operator (used for sort features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380e088d-44b2-4dd3-9679-a888dd9b2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db047fca-0340-48c5-9e9c-e901af6a9a75",
   "metadata": {},
   "source": [
    "## A)OBTAIN DATA\n",
    "\n",
    "---\n",
    "\n",
    "Dataset file (BBC) contains 6 files included README.TXT, the other 5 each for one catagory, for the first step I take all the text with their catagory index (from 0 to 4) orderly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d04838-6075-4bf7-ad6c-6ced73c0ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "['Claxton hunting first major medal\\n\\nBritish hurdler Sarah Claxton is confident she can win her first major medal at next month\\'s European Indoor Championships in Madrid.\\n\\nThe 25-year-old has already smashed the British record over 60m hurdles twice this season, setting a new mark of 7.96 seconds to win the AAAs title. \"I am quite confident,\" said Claxton. \"But I take each race as it comes. \"As long as I keep up my training but not do too much I think there is a chance of a medal.\" Claxton has won the national 60m hurdles title for the past three years but has struggled to translate her domestic success to the international stage. Now, the Scotland-born athlete owns the equal fifth-fastest time in the world this year. And at last week\\'s Birmingham Grand Prix, Claxton left European medal favourite Russian Irina Shevchenko trailing in sixth spot.\\n\\nFor the first time, Claxton has only been preparing for a campaign over the hurdles - which could explain her leap in form. In previous seasons, the 25-year-old also contested the long jump but since moving from Colchester to London she has re-focused her attentions. Claxton will see if her new training regime pays dividends at the European Indoors which take place on 5-6 March.\\n']\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path) # get bbc file\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines = fp.read()\n",
    "                    article = []\n",
    "                    text_ = \"\"\n",
    "                    text_ += lines\n",
    "                    article.append(text_)    \n",
    "                        \n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)\n",
    "print(all_text_list_[1313]) # the first element is the exact 001.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca914ab-1cf1-4c39-89cc-3858cfb85ce2",
   "metadata": {},
   "source": [
    "## B)TEXT PREPROCESSING\n",
    "\n",
    "---\n",
    "\n",
    "Split each word from each text and regulate them to lemma form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0a7ea3-13c4-4d84-830d-63ed13b531bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'sale', 'boost', 'time', 'warner', 'profit', 'quarterly', 'profit', 'at', 'us', 'medium', 'giant', 'timewarner', 'jumped', '76', '%', 'to', '$', '1.13bn', '(', 'â£600m', ')', 'for', 'the', 'three', 'month', 'to', 'december', ',', 'from', '$', '639m', 'year-earlier', '.', 'the', 'firm', ',', 'which', 'is', 'now', 'one', 'of', 'the', 'biggest', 'investor', 'in', 'google', ',', 'benefited', 'from', 'sale', 'of', 'high-speed', 'internet', 'connection', 'and', 'higher', 'advert', 'sale', '.', 'timewarner', 'said', 'fourth', 'quarter', 'sale', 'rose', '2', '%', 'to', '$', '11.1bn', 'from', '$', '10.9bn', '.', 'its', 'profit', 'were', 'buoyed', 'by', 'one-off', 'gain', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'warner', 'bros', ',', 'and', 'le', 'user', 'for', 'aol', '.', 'time', 'warner', 'said', 'on', 'friday', 'that', 'it', 'now', 'owns', '8', '%', 'of', 'search-engine', 'google', '.', 'but', 'it', 'own', 'internet', 'business', ',', 'aol', ',', 'had', 'ha', 'mixed', 'fortune', '.', 'it', 'lost', '464,000', 'subscriber', 'in', 'the', 'fourth', 'quarter', 'profit', 'were', 'lower', 'than', 'in', 'the', 'preceding', 'three', 'quarter', '.', 'however', ',', 'the', 'company', 'said', 'aol', \"'s\", 'underlying', 'profit', 'before', 'exceptional', 'item', 'rose', '8', '%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertising', 'revenue', '.', 'it', 'hope', 'to', 'increase', 'subscriber', 'by', 'offering', 'the', 'online', 'service', 'free', 'to', 'timewarner', 'internet', 'customer', 'and', 'will', 'try', 'to', 'sign', 'up', 'aol', \"'s\", 'existing', 'customer', 'for', 'high-speed', 'broadband', '.', 'timewarner', 'also', 'ha', 'to', 'restate', '2000', 'and', '2003', 'result', 'following', 'a', 'probe', 'by', 'the', 'us', 'securities', 'exchange', 'commission', '(', 'sec', ')', ',', 'which', 'is', 'close', 'to', 'concluding', '.', 'time', 'warner', \"'s\", 'fourth', 'quarter', 'profit', 'were', 'slightly', 'better', 'than', 'analyst', \"'\", 'expectation', '.', 'but', 'it', 'film', 'division', 'saw', 'profit', 'slump', '27', '%', 'to', '$', '284m', ',', 'helped', 'by', 'box-office', 'flop', 'alexander', 'and', 'catwoman', ',', 'a', 'sharp', 'contrast', 'to', 'year-earlier', ',', 'when', 'the', 'third', 'and', 'final', 'film', 'in', 'the', 'lord', 'of', 'the', 'rings', 'trilogy', 'boosted', 'result', '.', 'for', 'the', 'full-year', ',', 'timewarner', 'posted', 'a', 'profit', 'of', '$', '3.36bn', ',', 'up', '27', '%', 'from', 'it', '2003', 'performance', ',', 'while', 'revenue', 'grew', '6.4', '%', 'to', '$', '42.09bn', '.', '``', 'our', 'financial', 'performance', 'wa', 'strong', ',', 'meeting', 'or', 'exceeding', 'all', 'of', 'our', 'full-year', 'objective', 'and', 'greatly', 'enhancing', 'our', 'flexibility', ',', \"''\", 'chairman', 'and', 'chief', 'executive', 'richard', 'parsons', 'said', '.', 'for', '2005', ',', 'timewarner', 'is', 'projecting', 'operating', 'earnings', 'growth', 'of', 'around', '5', '%', ',', 'and', 'also', 'expects', 'higher', 'revenue', 'and', 'wider', 'profit', 'margin', '.', 'timewarner', 'is', 'to', 'restate', 'it', 'account', 'a', 'part', 'of', 'effort', 'to', 'resolve', 'an', 'inquiry', 'into', 'aol', 'by', 'us', 'market', 'regulator', '.', 'it', 'ha', 'already', 'offered', 'to', 'pay', '$', '300m', 'to', 'settle', 'charge', ',', 'in', 'a', 'deal', 'that', 'is', 'under', 'review', 'by', 'the', 'sec', '.', 'the', 'company', 'said', 'it', 'wa', 'unable', 'to', 'estimate', 'the', 'amount', 'it', 'needed', 'to', 'set', 'aside', 'for', 'legal', 'reserve', ',', 'which', 'it', 'previously', 'set', 'at', '$', '500m', '.', 'it', 'intends', 'to', 'adjust', 'the', 'way', 'it', 'account', 'for', 'a', 'deal', 'with', 'german', 'music', 'publisher', 'bertelsmann', \"'s\", 'purchase', 'of', 'a', 'stake', 'in', 'aol', 'europe', ',', 'which', 'it', 'had', 'reported', 'a', 'advertising', 'revenue', '.', 'it', 'will', 'now', 'book', 'the', 'sale', 'of', 'it', 'stake', 'in', 'aol', 'europe', 'a', 'a', 'loss', 'on', 'the', 'value', 'of', 'that', 'stake', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# take all the lemmatized words from each text as a new list\n",
    "def get_list_tokens(text_list):\n",
    "    paragraph = \",\".join(text_list)  # turn list into string\n",
    "    sentence_split=nltk.tokenize.sent_tokenize(paragraph) # get each sentence\n",
    "    \n",
    "    list_tokens=[]\n",
    "#     get each word from each sentence\n",
    "    for sentence in sentence_split:  \n",
    "        list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
    "        for token in list_tokens_sentence:\n",
    "            list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
    "    return list_tokens\n",
    "\n",
    "token = get_list_tokens(all_text_list_[0])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ec9b33-66d0-440f-9345-ccc82a860155",
   "metadata": {},
   "source": [
    "## C) TRAIN, DEVELOPMENT AND TEST SPLITS\n",
    "\n",
    "---\n",
    "\n",
    "Split all text into three different set: training set 80%, development set 10% and test set 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400067c1-6efe-4978-9088-a414b2b5021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size training set: 1780\n",
      "Size test set: 223\n",
      "Size development set: 222\n"
     ]
    }
   ],
   "source": [
    "# get full dataset in order to split our dataset into training and test\n",
    "# intergrate text and lable in a (text,lable) form\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "#     retrieve proportion of random datasets\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "# print (\"Size training set: \"+str(len(train_set)))\n",
    "# print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "# get test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "# shuffle each set to random sets\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b434ae-7701-416f-9d5f-0f72a6c5fed5",
   "metadata": {},
   "source": [
    "## D)FEATURE ENGINEERING\n",
    "\n",
    "---\n",
    "\n",
    "Select feature by three different ways: word frequency, Hash trick and Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3069514b-facb-4af5-8f4d-0e55f48f7ef8",
   "metadata": {},
   "source": [
    "### D.1)WORD FREQUENCY\n",
    "\n",
    "---\n",
    "\n",
    "Gater all the usefull words from all the text and make a vocabulary frequency dictionary, arrange the most used word to the least used word. So that every text will get a feature vector based on that vocabulary frequency dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea3960c-9dad-4aa4-888a-41f56a04fc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'said', 'wa', 'ha', 'mr', 'year', 'would', 'also', 'people', '%', 'new', 'one', 'us', 'could', 'game', 'last', 'time', 'first', 'say', \"n't\"]\n"
     ]
    }
   ],
   "source": [
    "# get the stopwords list from nltk and add some more\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",':','-']\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "def get_vocabulary(training_set, num_features): \n",
    "    dict_word_frequency={}\n",
    "    for instance in training_set:\n",
    "        sentence_tokens=get_list_tokens(instance[0])\n",
    "        for word in sentence_tokens:\n",
    "            if word in stopwords: continue\n",
    "            if word not in dict_word_frequency: dict_word_frequency[word]=1\n",
    "            else: dict_word_frequency[word]+=1\n",
    "    sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:num_features]\n",
    "    vocabulary=[]\n",
    "    for word,frequency in sorted_list:\n",
    "        vocabulary.append(word)\n",
    "    return vocabulary   #word frequency dictionary\n",
    "\n",
    "dic = get_vocabulary(dataset_full,1000)\n",
    "# show the top 15 \n",
    "print(dic[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226d1f3-c624-4cae-8d86-f2da0fcaf7b7",
   "metadata": {},
   "source": [
    "based on that vocabulary frequency dictionary, each text would able to get a feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acde7c45-31c9-490f-ab24-a97f7d88c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_text(list_vocab,text_list):\n",
    "#     based on the vocabulary frequency dictionary we get the vector size\n",
    "    vector_text=np.zeros(len(list_vocab))\n",
    "#     turn text(list) into string\n",
    "    paragraph = \",\".join(text_list)\n",
    "    list_tokens_string=get_list_tokens(paragraph)\n",
    "    for i, word in enumerate(list_vocab):\n",
    "        if word in list_tokens_string:\n",
    "            vector_text[i]=list_tokens_string.count(word)\n",
    "    return vector_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41989077-02a4-4377-b2fc-3ac0b78be4cd",
   "metadata": {},
   "source": [
    "### D.2)HASH TRICK\n",
    "\n",
    "---\n",
    "\n",
    "Finally, my last method used for feature extraction is the Hash trick, It is an outstanding method, because of its very low scalable to large datasets as there is no need to store a vocabulary dictionary in memory, by using HashingVectorizer from sklearn.feature_extraction.text , I changed the default feature number up to 500 to match the data size, generate then transform then we have the feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227cb2ad-d2ae-4f92-9b96-15e3f9c86215",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_hash_vec(training_set):\n",
    "    \n",
    "    hashing_vectorizer = HashingVectorizer(n_features = 500)\n",
    "    text_all_vec = hashing_vectorizer.transform(training_set).toarray()\n",
    "    \n",
    "    return text_all_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41e8de-7310-4e63-b408-87a8f6bc8b8f",
   "metadata": {},
   "source": [
    "### D.3)BIGRAM\n",
    "\n",
    "---\n",
    "\n",
    "The principle is to generate a dictionary with one and only sequence of two adjacent elements and words as the key, and their index as value. So that the feature vector would be very long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3ab31-0048-4c30-8391-b2e4259336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bigram_vocab(bigram_form_list):\n",
    "# get the corpus form text and generate a bigram dictionary\n",
    "    \n",
    "    bigram_vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 2))\n",
    "    text_all_vec = bigram_vectorizer.fit_transform(bigram_form_list)\n",
    "    \n",
    "    return bigram_vectorizer\n",
    "\n",
    "def get_bigram_vec(bigram_form_list, vectorizer):\n",
    "#     get the dictionary and the corpus form text to generate the whole vector\n",
    "        vec = vectorizer.transform(bigram_form_list).toarray()\n",
    "        vec = np.asarray(vec)\n",
    "        return vec\n",
    "    \n",
    "    \n",
    "\n",
    "def get_bigram_form(training_set):\n",
    "#     get the training set tuple and make it compatible for CountVectorizer\n",
    "    one_list = []\n",
    "    label = []\n",
    "    for instance in training_set:\n",
    "        article = ''.join(instance[0])\n",
    "        one_list.append(article)\n",
    "        label.append(instance[1])\n",
    "    \n",
    "    return one_list, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335604a4-ff69-4da1-931e-6ab69a0d2bb9",
   "metadata": {},
   "source": [
    "### D.4)CHI-SQUARE STATISTIC\n",
    "\n",
    "---\n",
    "\n",
    "Used for feature selection method to decrease the dimension.The principle is to test whether the observed frequency is significantly different from the expected frequency, If the chi-square value is larger, the degree of deviation between the two is larger; otherwise, the deviation between the two is smaller; if the two values are completely equal, the chi-square value is 0, indicating that the theoretical value is completely consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463561f3-e364-4874-9e80-650aadd5f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training eigenvectors and return a smaller and better eigenvectors after chi-square process\n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a0e98-45f8-4204-a035-eb9ec9be8fe5",
   "metadata": {},
   "source": [
    "## E)SVM TRAINING MODEL\n",
    "\n",
    "---\n",
    "\n",
    "Do the dimensionality reduction process before training the SVM classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b60e3e-26d4-449f-ba7d-b634f3536412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine three types of the dimensionality reduction method, feature number needed\n",
    "def train_svm_classifier(training_set, vocabulary, chi_num): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "        vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "        X_train.append(vector_instance)\n",
    "        Y_train.append(instance[1])\n",
    "        \n",
    "    fs_sentanalysis, X_train_chi_vector = chi_square_select(X_train,Y_train,chi_num)\n",
    "    \n",
    "    \n",
    "#   Finally, train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "#     train the last processed eigenvectors \n",
    "    svm_clf.fit(X_train_chi_vector,Y_train)\n",
    "    return svm_clf, fs_sentanalysis\n",
    "\n",
    "\n",
    "vocabulary = get_vocabulary(dataset_full, 10000)\n",
    "svm_clf, fs_sentanalysis = train_svm_classifier(new_train_set, vocabulary,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75950cf2-4cec-4076-a720-7fa52feb55d7",
   "metadata": {},
   "source": [
    "test our model in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b65c2b-551a-4f9a-b6d0-cfa7dfd3f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 0 0 4 1 2 4 3 2 0 0 1 3 1 1 0 1 0 2 3 3 2 0 2 3 4 3 0 0 3 3 1 3 3 4 1\n",
      " 3 0 3 0 3 3 0 0 1 4 0 1 4 0 1 0 3 3 0 0 1 1 3 3 4 0 2 2 0 4 3 2 4 1 4 0 4\n",
      " 0 0 1 4 3 4 0 3 2 4 2 3 2 2 3 0 3 4 0 1 3 1 0 4 0 2 3 2 3 0 4 0 2 2 2 1 3\n",
      " 0 0 1 0 1 3 0 2 2 3 1 1 0 3 0 4 2 0 0 1 2 1 2 1 2 0 4 1 1 0 4 1 3 0 2 4 2\n",
      " 3 4 3 0 2 3 1 0 1 3 0 3 4 1 4 4 2 0 0 4 0 0 3 2 4 3 1 1 0 0 1 4 4 3 3 4 3\n",
      " 3 2 0 1 1 4 3 2 1 1 0 0 3 2 2 1 3 0 4 3 3 3 3 2 1 3 0 2 0 4 0 3 2 2 0 0 1\n",
      " 2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77        54\n",
      "           1       0.57      0.61      0.59        38\n",
      "           2       0.81      0.73      0.77        41\n",
      "           3       0.77      0.78      0.78        51\n",
      "           4       0.76      0.67      0.71        39\n",
      "\n",
      "    accuracy                           0.73       223\n",
      "   macro avg       0.73      0.72      0.72       223\n",
      "weighted avg       0.73      0.73      0.73       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   \n",
    "    vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(fs_sentanalysis.transform(X_test))\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b3c1da8-a765-4b1b-95d0-5ef77f5b6ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.731\n",
      "Recall: 0.721\n",
      "F1-Score: 0.724\n",
      "Accuracy: 0.731\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score,accuracy_score\n",
    "\n",
    "precision=precision_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "recall=recall_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "f1=f1_score(Y_test_gold, Y_text_predictions, average='macro')\n",
    "accuracy=accuracy_score(Y_test_gold, Y_text_predictions)\n",
    "\n",
    "print (\"Precision: \"+str(round(precision,3)))\n",
    "print (\"Recall: \"+str(round(recall,3)))\n",
    "print (\"F1-Score: \"+str(round(f1,3)))\n",
    "print (\"Accuracy: \"+str(round(accuracy,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420581d-4d4a-48e1-aa03-dfd49ade10bd",
   "metadata": {},
   "source": [
    "Use development set to tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05707b2d-760e-4d61-ab02-8d7790a7d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev=[]\n",
    "for instance in new_dev_set:\n",
    "    Y_dev.append(instance[1])\n",
    "\n",
    "Y_dev_gold=np.asarray(Y_dev)\n",
    "\n",
    "# train our three models with the different number of features, and test each of them in the dev set\n",
    "\n",
    "list_num_features=[10000,12500,15000,17500]\n",
    "best_accuracy_dev=0.0\n",
    "for num_features in list_num_features:\n",
    "  # First, we get the vocabulary from the training set and train our svm classifier\n",
    "    vocabulary = get_vocabulary(dataset_full, num_features)\n",
    "    svm_clf, fs_sentanalysis, mfs_sentanalysis = train_svm_classifier(\n",
    "        new_train_set, vocabulary, round(num_features/3))\n",
    "\n",
    "\n",
    "  # Then, we transform our dev set into vectors and make the prediction on this set\n",
    "    X_dev=[]\n",
    "    for instance in new_dev_set:\n",
    "        vector_instance=get_vector_text(vocabulary,instance[0])\n",
    "        X_dev.append(vector_instance)\n",
    "    X_dev=np.asarray(X_dev)\n",
    "    \n",
    "    Y_dev_predictions = svm_clf.predict(mfs_sentanalysis.transform(fs_sentanalysis.transform(X_dev)))\n",
    "  # Finally, we get the accuracy results of the classifier\n",
    "    accuracy_dev=accuracy_score(Y_dev_gold, Y_dev_predictions)\n",
    "    print (\"Accuracy with \"+str(num_features)+\": \"+str(round(accuracy_dev,3)))\n",
    "    if accuracy_dev>=best_accuracy_dev:\n",
    "        best_accuracy_dev=accuracy_dev\n",
    "        best_num_features=num_features\n",
    "        best_vocabulary=vocabulary\n",
    "        best_svm_clf=svm_clf\n",
    "print (\"\\n Best accuracy overall in the dev set is \"+str(round(best_accuracy_dev,3))+\" with \"+str(best_num_features)+\" features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f925e88e-31db-461a-8bf1-db7728b40181",
   "metadata": {},
   "source": [
    "## E)SVM TRAINING MODEL\n",
    "\n",
    "---\n",
    "\n",
    "Do the dimensionality reduction process before training the SVM classifier and combine the feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e68715-54c5-4d34-a3e1-8523c77bb4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_classifier_combine(training_set, vocabulary, chi_num): # Function for training our svm classifier\n",
    "    X_train=[]\n",
    "    Y_train=[]\n",
    "    for instance in training_set:\n",
    "#         bigram function unfortunately broke down, still can't fix it\n",
    "        vector_fre = get_vector_text(vocabulary,instance[0])# get frequency vector\n",
    "        vector_hash = get_hash_vec(instance[0])# get hash vector\n",
    "        vec_mul = append_list(vector_hash, vector_fre)# join vectors together\n",
    "        vec_mul = minmax_scale(vec_mul)# minmax scale the vector\n",
    "        \n",
    "        X_train.append(vec_mul)\n",
    "        \n",
    "        Y_train.append(instance[1])\n",
    "    \n",
    "    X_train = np.asarray(X_train)\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    # turn 3Dim into 2Dim matrix\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    d2_X_train = X_train.reshape((nsamples,nx*ny))\n",
    "    \n",
    "    fs_sentanalysis, X_train_chi_vector = chi_square_select(d2_X_train,Y_train,chi_num)# chi-square \n",
    "    \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "    svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "    svm_clf.fit(X_train_chi_vector,Y_train)\n",
    "    return svm_clf, fs_sentanalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocabulary = get_vocabulary(new_train_set, 1000)\n",
    "svm_clf, fs_sentanalysis = train_svm_classifier_combine(new_train_set, vocabulary,500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
