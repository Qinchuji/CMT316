{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6af26019-8901-4439-ac6a-07c7d452df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'entertainment', 'politics', 'README.TXT', 'sport', 'tech']\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\business\n",
      "510\n",
      "business\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\entertainment\n",
      "386\n",
      "entertainment\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\politics\n",
      "417\n",
      "politics\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\README.TXT\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\sport\n",
      "511\n",
      "sport\n",
      "C:\\Users\\c2008016\\Desktop\\Programs\\coursework\\bbc\\tech\n",
      "401\n",
      "tech\n",
      "Size training set: 1780\n",
      "Size test set: 445\n",
      "Size training set: 1780\n",
      "Size test set: 223\n",
      "Size development set: 222\n",
      "(1780, 20)\n",
      "[3 1 0 2 4 3 3 3 3 1 3 3 0 0 2 4 4 4 1 3 3 3 3 4 3 3 3 0 4 3 4 2 0 0 1 0 0\n",
      " 4 2 0 4 3 1 3 0 3 1 4 0 3 0 1 1 4 3 3 0 0 0 4 0 1 4 3 0 4 3 4 1 2 2 2 0 1\n",
      " 2 2 3 3 2 4 0 2 0 3 3 3 0 2 0 3 0 1 0 2 4 3 3 4 0 4 2 2 0 0 3 1 0 2 1 0 3\n",
      " 1 2 0 2 2 0 0 3 4 2 4 4 0 1 3 3 4 4 1 1 1 4 3 3 2 4 2 4 2 3 0 4 4 1 0 0 3\n",
      " 0 4 0 4 1 3 0 2 4 4 2 4 3 0 4 4 2 3 0 3 4 0 1 4 3 1 4 2 0 0 4 0 3 4 0 2 1\n",
      " 2 0 4 0 3 1 3 2 1 2 0 3 2 2 4 1 1 0 4 2 0 0 0 4 3 2 4 0 0 2 1 4 3 1 4 1 2\n",
      " 0]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.60      0.57        50\n",
      "           1       0.52      0.40      0.45        40\n",
      "           2       0.57      0.60      0.58        35\n",
      "           3       0.45      0.61      0.52        38\n",
      "           4       0.67      0.53      0.59        60\n",
      "\n",
      "    accuracy                           0.55       223\n",
      "   macro avg       0.55      0.55      0.54       223\n",
      "weighted avg       0.56      0.55      0.55       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#这个是对的\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['/','(',')','{','}','@','|',';','\\n','#','+','_','.',',','``',\"''\",':','-']\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "path = os.getcwd() + \"\\\\bbc\"\n",
    "list_catagory_files = os.listdir(path)\n",
    "print(list_catagory_files)\n",
    "num_catagory = len(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "def text_label_list(cata_path):\n",
    "    label = 0\n",
    "    all_text_list = []\n",
    "    all_label_list = []\n",
    "    catagory_file_num = []\n",
    "    \n",
    "    for catagory in list_catagory_files:\n",
    "        catagory_file = path + \"\\\\\" + catagory\n",
    "        print(catagory_file)\n",
    "        if catagory_file.endswith('.TXT'):\n",
    "            continue\n",
    "        else:\n",
    "        \n",
    "            file_num = len(os.listdir(catagory_file))\n",
    "            catagory_file_num.append(file_num)\n",
    "            print(file_num)\n",
    "            print(catagory)\n",
    "            for i in range(1,file_num + 1):\n",
    "                all_label_list.append(label)\n",
    "                j = \"%03d\" % i\n",
    "                with open(\"\\\\\".join([catagory_file, str(j) + \".txt\"])) as fp:\n",
    "                    lines = fp.read()\n",
    "                    article = []\n",
    "                    text_ = \"\"\n",
    "                    text_ += lines\n",
    "                    article.append(text_)    \n",
    "                        \n",
    "                        \n",
    "                    all_text_list.append(article)\n",
    "        label += 1\n",
    "    return all_text_list, all_label_list, catagory_file_num\n",
    "\n",
    "all_text_list_, all_label_list_, catagory_file_num = text_label_list(list_catagory_files)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# get full dataset in order to split our dataset into training and test\n",
    "dataset_full=[] \n",
    "for text, label in zip(all_text_list_, all_label_list_):\n",
    "    dataset_full.append((text,label))\n",
    "        \n",
    "\n",
    "def get_train_test_split(dataset_full,ratio):\n",
    "    pre_train_set=[]\n",
    "    pre_test_set=[]\n",
    "    \n",
    "    size_dataset_full = len(dataset_full)\n",
    "    test_size = int(round(size_dataset_full * ratio))\n",
    "    list_test_indices=random.sample(range(size_dataset_full), test_size)\n",
    "    \n",
    "    for i,text in enumerate(dataset_full):\n",
    "        if i in list_test_indices:\n",
    "            pre_test_set.append(text)\n",
    "        else:pre_train_set.append(text)\n",
    "        \n",
    "    return pre_train_set,pre_test_set\n",
    "# get training set, test set\n",
    "train_set, test_set = get_train_test_split(dataset_full,0.2)\n",
    "print (\"Size training set: \"+str(len(train_set)))\n",
    "print (\"Size test set: \"+str(len(test_set)))\n",
    "\n",
    "\n",
    "# get training set, test set, development set\n",
    "new_test_set, new_dev_set = get_train_test_split(test_set,0.5)\n",
    "\n",
    "new_train_set=train_set\n",
    "random.shuffle(new_train_set)\n",
    "random.shuffle(new_dev_set)\n",
    "random.shuffle(new_test_set)\n",
    "print (\"Size training set: \"+str(len(new_train_set)))\n",
    "print (\"Size test set: \"+str(len(new_test_set)))\n",
    "print (\"Size development set: \"+str(len(new_dev_set)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_hash_vec(training_set):\n",
    "    \n",
    "    hashing_vectorizer = HashingVectorizer(n_features = 20)\n",
    "    text_all_vec = hashing_vectorizer.transform(training_set).toarray()\n",
    "    \n",
    "    return text_all_vec\n",
    "\n",
    "\n",
    "        \n",
    "def chi_square_select(X_train,Y_train,num_features):\n",
    "    fs_sentanalysis = SelectKBest(chi2, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = fs_sentanalysis.transform(X_train)\n",
    "    return fs_sentanalysis, X_train_new\n",
    "\n",
    "\n",
    "def mutual_info_select(X_train,Y_train,num_features):\n",
    "    mfs_sentanalysis = SelectKBest(mutual_info_classif, k=num_features).fit(X_train, Y_train)\n",
    "    X_train_new = mfs_sentanalysis.transform(X_train)\n",
    "    return mfs_sentanalysis, X_train_new\n",
    "    \n",
    "\n",
    "\n",
    "# def train_svm_classifier(training_set, bigram_vectorizer, chi_num, mutual_num): # Function for training our svm classifier\n",
    "#     X_train=[]\n",
    "#     Y_train=[]\n",
    "#     for instance in training_set:\n",
    "#         vector_instance_bigram=get_bigram_vec(instance[0],bigram_vectorizer)\n",
    "#         X_train.append(vector_instance_bigram)\n",
    "#         Y_train.append(instance[1])\n",
    "        \n",
    "#     fs_sentanalysis, X_train_chi_vector = chi_square_select(X_train,Y_train,chi_num)\n",
    "#     mfs_sentanalysis, X_train_mut_vector = mutual_info_select(X_train_chi_vector,Y_train,mutual_num)\n",
    "    \n",
    "#   # Finally, we train the SVM classifier \n",
    "#     svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "#     svm_clf.fit(X_train_mut_vector,Y_train)\n",
    "#     return svm_clf, fs_sentanalysis, mfs_sentanalysis\n",
    "\n",
    "\n",
    "# # print(all_text_list[-1:])\n",
    "# print(all_text_list_[1313])\n",
    "# print(all_label_list_[1313])\n",
    "\n",
    "\n",
    "# def train_svm_classifier_test(tra_set): # Function for training our svm classifier\n",
    "X_train_=[]\n",
    "Y_train_=[]\n",
    "for instance in new_train_set:\n",
    "        \n",
    "    vector_instance_hash =get_hash_vec(instance[0])\n",
    "#         vector_instance_hash = vector_instance_hash.reshape(-1, 1)\n",
    "    X_train_.append(vector_instance_hash)\n",
    "    Y_train_.append(instance[1])\n",
    "        \n",
    "\n",
    "X_train_=np.asarray(X_train)\n",
    "Y_train_=np.asarray(Y_train)\n",
    "nsamples, nx, ny = X_train.shape\n",
    "d2_X_train = X_train.reshape((nsamples,nx*ny))\n",
    "# X_train_= X_train_.reshape(-1,1)    \n",
    "print(d2_X_train.shape)\n",
    "# print(Y_train.shape)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "  # Finally, we train the SVM classifier \n",
    "svm_clf=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
    "svm_clf.fit(d2_X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# svm_clf, fs_sentanalysis, mfs_sentanalysis = train_svm_classifier(new_train_set, bigram_vec,500, 250)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test=[]\n",
    "Y_test=[]\n",
    "for instance in new_test_set:   #注意是test_set,上面的描述也错了\n",
    "    vector_instance=get_hash_vec(instance[0])\n",
    "    X_test.append(vector_instance)\n",
    "    Y_test.append(instance[1])\n",
    "X_test=np.asarray(X_test)\n",
    "nsamples_, nx_, ny_ = X_test.shape\n",
    "d2_X_test = X_test.reshape((nsamples_,nx*ny_))\n",
    "Y_test_gold=np.asarray(Y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Y_text_predictions = svm_clf.predict(d2_X_test)\n",
    "print(Y_text_predictions)\n",
    "print(classification_report(Y_test_gold, Y_text_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "804496e5-1775-4794-a32f-24fbcf41e52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.1221125   0.39686563 -0.15264063  0.1221125  -0.18316875 -0.244225\n",
      "   0.21369688  0.15264063 -0.1221125   0.33580938 -0.18316875  0.39686563\n",
      "   0.18316875  0.09158438 -0.15264063  0.03052813  0.27475313 -0.03052813\n",
      "  -0.39686563 -0.09158438]]\n",
      "(1, 20)\n",
      "[[ 0.04658412  0.12810632  0.11646029 -0.17469044 -0.09316824 -0.11646029\n",
      "  -0.04658412 -0.03493809  0.1979825   0.24456662 -0.06987618  0.31444279\n",
      "  -0.01164603 -0.03493809  0.09316824 -0.02329206  0.43090309  0.13975235\n",
      "  -0.69876177  0.06987618]]\n",
      "(1780, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vecc = get_hash_vec(all_text_list_[0])\n",
    "print(vecc)\n",
    "print(vecc.shape)\n",
    "X_train = []\n",
    "Y_train = []\n",
    "for instance in new_train_set:\n",
    "    vector_instance_hash =get_hash_vec(instance[0])\n",
    "#         vector_instance_hash = vector_instance_hash.reshape(-1, 1)\n",
    "    X_train.append(vector_instance_hash)\n",
    "    Y_train.append(instance[1])\n",
    "\n",
    "X_train=np.asarray(X_train)\n",
    "print(X_train[0])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d34c1c-ccf3-4455-8658-8ddb11f3ac55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
